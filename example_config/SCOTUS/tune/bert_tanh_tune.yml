 # Yu-Chen's: https://github.com/JamesLYC88/text_classification_baseline_code/blob/main/config/scotus/bert_tune.yml
# data
training_file: data/SCOTUS/train.txt
val_file: data/SCOTUS/valid.txt
test_file: data/SCOTUS/test.txt
data_name: SCOTUS
min_vocab_freq: 1
max_seq_length: 512
include_test_labels: true
remove_no_label_data: false
add_special_tokens: true

# train
seed: 1337
epochs: 15
batch_size: 8
optimizer: adamw
learning_rate: ['grid_search', [0.00002, 0.00003, 0.00005]]
weight_decay: ['grid_search', [0, 0.001]]
patience: 5
shuffle: true

# eval
eval_batch_size: 8
monitor_metrics: ['Macro-F1', 'Micro-F1', 'P@1', 'P@5', 'nDCG@5']
early_stopping_metric: Micro-F1
val_metric: Micro-F1

# model
model_name: BERTLWAN_exps
init_weight: null
loss_function: binary_cross_entropy_with_logits
network_config:
  attn_mode: tanh
  output_mode: vanilla
  # https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#transformers.BertConfig
  # The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.
  encoder_hidden_dropout: ['grid_search', [0.1, 0.2]] # hidden_dropout_prob
  lm_weight: bert-base-uncased


# pretrained vocab / embeddings
embed_file: null
vocab_file: null
normalize_embed: false

# hyperparamter search
search_alg: basic_variant
embed_cache_dir: .vector_cache
num_samples: 1
scheduler: null
checkpoint_path: null
cpu: false
data_workers: 4
eval: false
label_file: null
limit_train_batches: 1.0
limit_val_batches: 1.0
limit_test_batches: 1.0
metric_threshold: 0.5
result_dir: runs
save_k_predictions: 0
silent: true
val_size: 0.2
lr_scheduler: null
scheduler_config: null
momentum: 0